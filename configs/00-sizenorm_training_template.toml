[paths]
training='/n/groups/datta/win/longtogeny/data/size_network/training_data/3month-male-multi_animal_training_data.h5'
validation='/n/groups/datta/win/longtogeny/data/size_network/male_ontogeny_validation.h5'
age_validation='/n/groups/datta/win/longtogeny/data/size_network/male_ontogeny_pose_balanced_validation.h5'

wall_noise='/n/groups/datta/win/longtogeny/data/size_network/training_data/wall_noise.h5'
saving='/n/groups/datta/win/longtogeny/size_norm/models/pre_final_model-2023-09-30_v00'

[augmentation]
angle_range = [-65, 65]
scale_x = [0.25, 2.0]
scale_y = [0.25, 2.0]
scale_z = [0.7, 1.3]
translate_x = 0.175  # proportional
translate_y = 0.25  # proportional
height = [-7, 7]
white_noise_scale = 3
min_threshold_range = [1, 17]
max_threshold_range = [50, 120]

clean_prob = 0.1
tail_ksize_range = [3, 15]
dilation_ksize_range = [3, 11]
morph_height_thresh_range = [5, 25]
dilation_prob = 0.5

elastic_alpha = 1.75
elastic_sigma = 9.0
elastic_ksize = 61
elastic_flag = true

rotation_prob = 0.7
translation_prob = 0.7
scale_prob = 0.6
shear_prob = 0.1
threshold_prob = 0.05
height_offset_prob = 0.6
height_scale_prob = 0.4
preserve_aspect_prob = 0.8
white_noise_prob = 0.2
scaled_white_noise_prob = 0.25
flip_prob = 0.5
random_elastic_prob = 0.2
tps_warp_prob = 0.7
scale_and_warp_prob = 0.166  # multiplied with scale_prob, this becomes 0.1
wall_reflection_prob = 0.15
zoom_unzoom_prob = 0.2

seed = 0

[callbacks.dynamics_validation]
log_interval = 1000  # epochs
max_frames = 6000

[callbacks.age_validation]
log_interval = 1000  # epochs
max_frames = 6000

[callbacks.stopping]
patience = 250  # epochs

[callbacks.finetuning]  # really this means freeze the decoder
finetune = false
freeze_epoch = 60

[trainer]
max_epochs = 600

[model]
depth = 4
init_channel = 64
input_channel = 32
init_depth = 1
channel_scaling = 2
depth_scaling = 1.0
activation = 'GELU'

[model.lightning]
adversarial_prob = 0.01
lr = 0.001
weight_decay = 0.00001
arch = 'ae'  # ae, unet, efficient, efficient_unet, vae
train_adversarial = false
adversarial_age_lr = 1e-4
adversarial_arch = "nonlinear"  # linear, nonlinear
adversarial_type = "continuous"  # class, continuous
adversarial_init_epoch = 25
max_grad = 100  # gradient clipping
use_l2_regularization_class = false
l2_regularization_class_alpha = 5e-4
use_l2_regularization_sn = false
l2_regularization_sn_alpha = 5e-4
use_curriculum_learning = false
curriculum_blocks = [30000, 40000, 50000]  # currently 466 steps per epoch

[model.lightning.lr_scheduler_params]
patience = 125

# scaling magnitude of age-based adversarial loss
[model.lightning.adversarial_age_scaling]
start_epoch = 30
start_value = 1e-7
max_value = 2e-4
epochs_to_max = 30

[model.lightning.vae_loss_params]
beta = 5
kl_factor = 1
gauss_std = 0.0115

[model.ae]
double_conv = true
separable = true
residual = true
img_size = 80
bottleneck = 25
bottleneck_channels = 64
# new parameter
dropout_p = 0.2
use_fft_branch = false

[model.unet]
double_conv = true
separable = true
residual = false

[model.efficient]
fused = 1
expansion = 4

[model.efficient_unet]
fused = 1
expansion = 4

[model.vae]
double_conv = true
separable = true
residual = true
img_size = 80
bottleneck = 25
bottleneck_channels = 64
# new parameter
dropout_p = 0.2
use_fft_branch = false
