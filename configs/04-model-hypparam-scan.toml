[paths]
# saving = '/n/groups/datta/win/longtogeny/size_norm/models/bottleneck_optimization_00'
saving = '/n/groups/datta/win/longtogeny/size_norm/models/freeze_decoder_00'

[stage1.model]
arch = ['ae', 'vae']
init_channel = [16, 32, 64, 128, 256, 512]
double_conv = [false]
bottleneck = [5, 9, 25, 50]

[stage2.model]
arch = ['vae']
use_curriculum_learning = [false, true]
init_channel = [128]
bottleneck = [25]
double_conv = [true]


[stage3.model]
arch = ['vae']
init_channel = [128, 256, 512]
double_conv = [true]
bottleneck = [25]


[stage4.model]
arch = ['vae']
init_channel = [128]
bottleneck = [25]
double_conv = [true]
use_curriculum_learning = [false, true]
use_fft_branch = [false, true]


[stage5.model]
arch = ['vae', 'ae']
init_channel = [128]
bottleneck = [25]
double_conv = [true]
use_curriculum_learning = [false, true]
use_fft_branch = [false, true]

[stage6.model]
arch = ['vae', 'ae']
init_channel = [128, 256, 512]
double_conv = [true]
bottleneck = [25]
use_curriculum_learning = [true]

# this one is to test freezing the decoder after 30 epochs - doesn't work like I thought
[stage7.model]
arch = ['vae']
init_channel = [64]
bottleneck = [10, 25]
double_conv = [true]
use_curriculum_learning = [false]

# this one is to test removing freezing and curriculum learning
[stage8.model]
arch = ['vae', 'ae']
init_channel = [64]
bottleneck = [10, 25]
double_conv = [true]
use_curriculum_learning = [false]

[stage9.model]
arch = ['vae', 'ae']
init_channel = [128, 256, 512]
double_conv = [true]
bottleneck = [9, 25, 50]
use_curriculum_learning = [false]