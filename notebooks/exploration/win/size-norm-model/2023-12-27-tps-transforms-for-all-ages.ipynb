{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize morphological transformation vectors to map 12w old mice onto all other ages\n",
    "\n",
    "Run this notebook on a gpu for fast training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from kornia.geometry.transform import scale\n",
    "from aging.plotting import figure, format_plots\n",
    "from aging.organization.paths import TrainingPaths\n",
    "from toolz import keyfilter, concatv, take, partition_all\n",
    "from aging.size_norm.data import make_grid, pad_vector\n",
    "from kornia.geometry.transform import warp_image_tps, get_tps_transform, resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_paths = TrainingPaths()\n",
    "training_paths.tps_fits.parent.mkdir(exist_ok=True, parents=True)\n",
    "poses = joblib.load(training_paths.tps_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(grid_size=6, batch_size=1):\n",
    "    movement_vector = torch.zeros(batch_size, 2, grid_size - 2, grid_size - 2, requires_grad=True)\n",
    "    scale_tensor = torch.ones(batch_size, 2, dtype=torch.float, requires_grad=True)\n",
    "    height_mtx = torch.ones(batch_size, 1, 5, 8, requires_grad=True)\n",
    "    return {\n",
    "        \"movement_vector\": movement_vector,\n",
    "        \"scale_tensor\": scale_tensor,\n",
    "        \"height_mtx\": height_mtx,\n",
    "    }\n",
    "\n",
    "\n",
    "def fit(template, target, grid_size=6, num_iters=1000, lr=1e-2, **tqdm_kwargs):\n",
    "    batch_size = len(template)\n",
    "    template = torch.tensor(template, dtype=torch.float).view(batch_size, 1, *template.shape[-2:])\n",
    "    target = torch.tensor(target, dtype=torch.float).view(batch_size, 1, *target.shape[-2:])\n",
    "\n",
    "    grid = make_grid(grid_size, batch_size)\n",
    "    params = initialize_params(grid_size, batch_size)\n",
    "    optimizer = torch.optim.AdamW(params.values(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "    losses = []\n",
    "    for i in tqdm(range(num_iters), **tqdm_kwargs):\n",
    "        optimizer.zero_grad()\n",
    "        # TODO: try elastic transform instead\n",
    "        padded_vector = torch.transpose(pad_vector(params[\"movement_vector\"]), 1, 3)\n",
    "        kernel, affine = get_tps_transform(grid + padded_vector.reshape(batch_size, -1, 2), grid)\n",
    "        scaled_template = scale(template, params[\"scale_tensor\"])\n",
    "        transformed_template = warp_image_tps(scaled_template, grid, kernel, affine)\n",
    "        height_intermediate = resize(params[\"height_mtx\"], target.shape[-2:])\n",
    "\n",
    "        out = height_intermediate * transformed_template\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return {\n",
    "        \"params\": {k: v.detach().squeeze().numpy() for k, v in params.items()},\n",
    "        \"losses\": losses,\n",
    "        \"optimized_template\": out.detach().squeeze().numpy().astype('uint8'),\n",
    "        \"template\": template.detach().squeeze().numpy().astype('uint8'),\n",
    "        \"target\": target.detach().squeeze().numpy().astype('uint8'),\n",
    "        \"grid\": grid.detach().squeeze().numpy(),\n",
    "        \"transformed_grid\": (\n",
    "            grid + torch.transpose(pad_vector(params[\"movement_vector\"]), 1, 3).reshape(batch_size, -1, 2)\n",
    "        )\n",
    "        .detach()\n",
    "        .squeeze()\n",
    "        .numpy(),\n",
    "    }\n",
    "\n",
    "\n",
    "def flatten_dict_recursive(input_dict, top_key=None):\n",
    "    flattened_list = []\n",
    "\n",
    "    for key, value in input_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            # If the value is a dictionary, recursively call the function\n",
    "            flattened_list.extend(\n",
    "                flatten_dict_recursive(\n",
    "                    value, top_key=key if top_key is None else top_key\n",
    "                )\n",
    "            )\n",
    "        elif isinstance(value, list):\n",
    "            # If the value is a list, iterate over its elements and append to the result\n",
    "            for item in value:\n",
    "                flattened_list.append((top_key, key, item))\n",
    "        else:\n",
    "            # If the value is neither a dictionary nor a list, append to the result\n",
    "            flattened_list.append((top_key, key, value))\n",
    "\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "pose_list = flatten_dict_recursive(keyfilter(lambda k: k != 12, poses))\n",
    "# 8 new animals, and 8 old animals\n",
    "template_pose_list = list(poses[12].items())\n",
    "random.shuffle(template_pose_list)\n",
    "n_animals = 13\n",
    "full_list = list(product(pose_list, template_pose_list[:n_animals]))\n",
    "random.shuffle(full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d46bb0fa41418a82f634d15e975e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m template_paths \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m grp]\n\u001b[1;32m      9\u001b[0m templates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m grp])\n\u001b[0;32m---> 10\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     12\u001b[0m     output[ages[j]][(paths[j], template_paths[j])] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement_vector\u001b[39m\u001b[38;5;124m'\u001b[39m: out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement_vector\u001b[39m\u001b[38;5;124m'\u001b[39m][j],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformed_grid\u001b[39m\u001b[38;5;124m'\u001b[39m: out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformed_grid\u001b[39m\u001b[38;5;124m'\u001b[39m][j]\n\u001b[1;32m     24\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(template, target, grid_size, num_iters, lr, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m padded_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(pad_vector(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovement_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     25\u001b[0m kernel, affine \u001b[38;5;241m=\u001b[39m get_tps_transform(grid \u001b[38;5;241m+\u001b[39m padded_vector\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), grid)\n\u001b[0;32m---> 26\u001b[0m scaled_template \u001b[38;5;241m=\u001b[39m \u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscale_tensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m transformed_template \u001b[38;5;241m=\u001b[39m warp_image_tps(scaled_template, grid, kernel, affine)\n\u001b[1;32m     28\u001b[0m height_intermediate \u001b[38;5;241m=\u001b[39m resize(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight_mtx\u001b[39m\u001b[38;5;124m\"\u001b[39m], target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/aging/lib/python3.10/site-packages/kornia/geometry/transform/affwarp.py:462\u001b[0m, in \u001b[0;36mscale\u001b[0;34m(tensor, scale_factor, center, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m    459\u001b[0m scaling_matrix: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m _compute_scaling_matrix(scale_factor, center)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# warp using the affine transform\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maffine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aging/lib/python3.10/site-packages/kornia/geometry/transform/affwarp.py:162\u001b[0m, in \u001b[0;36maffine\u001b[0;34m(tensor, matrix, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m    160\u001b[0m height: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    161\u001b[0m width: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 162\u001b[0m warped: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mwarp_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# return in the original shape\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_unbatched:\n",
      "File \u001b[0;32m~/miniconda3/envs/aging/lib/python3.10/site-packages/kornia/geometry/transform/imgwarp.py:198\u001b[0m, in \u001b[0;36mwarp_affine\u001b[0;34m(src, M, dsize, mode, padding_mode, align_corners, fill_value)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# src_norm_trans_dst_norm = torch.inverse(dst_norm_trans_src_norm)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m src_norm_trans_dst_norm \u001b[38;5;241m=\u001b[39m _torch_inverse_cast(dst_norm_trans_src_norm)\n\u001b[0;32m--> 198\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_norm_trans_dst_norm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdsize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdsize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _fill_and_warp(src, grid, align_corners\u001b[38;5;241m=\u001b[39malign_corners, mode\u001b[38;5;241m=\u001b[39mmode, fill_value\u001b[38;5;241m=\u001b[39mfill_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/aging/lib/python3.10/site-packages/torch/nn/functional.py:4399\u001b[0m, in \u001b[0;36maffine_grid\u001b[0;34m(theta, size, align_corners)\u001b[0m\n\u001b[1;32m   4396\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(size) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected non-zero, positive output size. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_grid_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = defaultdict(dict)\n",
    "grid_size = 10\n",
    "batch_size = 8\n",
    "for i, grp in enumerate(partition_all(batch_size, tqdm(full_list))):\n",
    "    ages = [i[0][0] for i in grp]\n",
    "    paths = [i[0][1] for i in grp]\n",
    "    targets = np.array([i[0][2] for i in grp])\n",
    "    template_paths = [i[1][0] for i in grp]\n",
    "    templates = np.array([i[1][1] for i in grp])\n",
    "    out = fit(templates, targets, grid_size=grid_size, num_iters=150, lr=1e-2, disable=True)\n",
    "    for j in range(batch_size):\n",
    "        output[ages[j]][(paths[j], template_paths[j])] = {\n",
    "            'params': {\n",
    "                'movement_vector': out['params']['movement_vector'][j],\n",
    "                'scale_tensor': out['params']['scale_tensor'][j],\n",
    "                'height_mtx': out['params']['height_mtx'][j],\n",
    "            },\n",
    "            'losses': out['losses'],\n",
    "            'optimized_template': out['optimized_template'][j],\n",
    "            'template': out['template'][j],\n",
    "            'target': out['target'][j],\n",
    "            'grid': out['grid'][j],\n",
    "            'transformed_grid': out['transformed_grid'][j]\n",
    "        }\n",
    "    if (i + 1) % 100 == 0:\n",
    "        joblib.dump(output, training_paths.tps_fits, compress=3)\n",
    "joblib.dump(output, training_paths.tps_fits, compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ontogeny",
   "language": "python",
   "name": "aging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
