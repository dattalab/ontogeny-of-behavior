{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to train ARHMM\n",
    "\n",
    "- find optimal kappa\n",
    "- use kappa to fit full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg41/miniconda3/envs/jax-moseq-og/lib/python3.10/site-packages/fastprogress/fastprogress.py:107: UserWarning: Couldn't import ipywidgets properly, progress bar will use console behavior\n",
      "  warn(\"Couldn't import ipywidgets properly, progress bar will use console behavior\")\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=.9\n",
    "import jax\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax_moseq.utils import set_mixed_map_iters, set_mixed_map_gpus\n",
    "\n",
    "import h5py\n",
    "import joblib\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from toolz import valmap, valfilter, keyfilter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax_moseq.models import arhmm\n",
    "from jax_moseq.utils import batch, convert_data_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def whiten_all(data_dict, center=True):\n",
    "    \"\"\"\n",
    "    Whiten the PC Scores (with Cholesky decomposition) using all the data to compute the covariance matrix.\n",
    "\n",
    "    Args:\n",
    "    data_dict (OrderedDict): Training dataset\n",
    "    center (bool): Indicates whether to center data by subtracting the mean PC score.\n",
    "\n",
    "    Returns:\n",
    "    data_dict (OrderedDict): Whitened training data dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    non_nan = lambda x: x[~np.isnan(np.reshape(x, (x.shape[0], -1))).any(1)]\n",
    "    meancov = lambda x: (x.mean(0), np.cov(x, rowvar=False, bias=1))\n",
    "    contig = partial(np.require, dtype=np.float64, requirements='C')\n",
    "\n",
    "    mu, Sigma = meancov(np.concatenate(list(map(non_nan, data_dict.values()))))\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "\n",
    "    offset = 0. if center else mu\n",
    "    apply_whitening = lambda x:  np.linalg.solve(L, (x-mu).T).T + offset\n",
    "\n",
    "    return OrderedDict((k, contig(apply_whitening(v))) for k, v in data_dict.items())\n",
    "\n",
    "\n",
    "def concatenate_stateseqs(stateseqs, mask=None):\n",
    "    \"\"\"\n",
    "    Concatenate state sequences, optionally applying a mask.\n",
    "    Parameters\n",
    "    ----------\n",
    "    stateseqs: dict or ndarray, shape (..., t)\n",
    "        Dictionary mapping names to 1d arrays, or a single\n",
    "        multi-dimensional array representing a batch of state sequences\n",
    "        where the last dim indexes time\n",
    "    mask: ndarray, shape (..., >=t), default=None\n",
    "        Binary indicator for which elements of ``stateseqs`` are valid,\n",
    "        e.g. when state sequences of different lengths have been padded.\n",
    "        If ``mask`` contains more time-points than ``stateseqs``, the\n",
    "        initial extra time-points will be ignored.\n",
    "    Returns\n",
    "    -------\n",
    "    stateseqs_flat: ndarray\n",
    "        1d array containing all state sequences \n",
    "    \"\"\"\n",
    "    if isinstance(stateseqs, dict):\n",
    "        stateseq_flat = np.hstack(list(stateseqs.values()))\n",
    "    elif mask is not None:\n",
    "        stateseq_flat = stateseqs[mask[:, -stateseqs.shape[1]:] > 0]\n",
    "    else:\n",
    "        stateseq_flat = stateseqs.flatten()\n",
    "    return stateseq_flat\n",
    "\n",
    "\n",
    "def get_durations(stateseqs, mask=None):\n",
    "    \"\"\"\n",
    "    Get durations for a batch of state sequences. For a more detailed \n",
    "    description of the function parameters, see \n",
    "    :py:func:`keypoint_moseq.util.concatenate_stateseqs`\n",
    "    Parameters\n",
    "    ----------\n",
    "    stateseqs: dict or ndarray of shape (..., t)\n",
    "    mask: ndarray of shape (..., >=t), default=None\n",
    "    Returns\n",
    "    -------\n",
    "    durations: 1d array\n",
    "        The duration of each each state (across all state sequences)\n",
    "    Examples\n",
    "    --------\n",
    "    >>> stateseqs = {\n",
    "        'name1': np.array([1, 1, 2, 2, 2, 3]),\n",
    "        'name2': np.array([0, 0, 0, 1])\n",
    "    }\n",
    "    >>> get_durations(stateseqs)\n",
    "    array([2, 3, 1, 3, 1])\n",
    "    \"\"\"\n",
    "    stateseq_flat = concatenate_stateseqs(stateseqs, mask=mask).astype(int)\n",
    "    stateseq_padded = np.hstack([[-1], stateseq_flat, [-1]])\n",
    "    changepoints = np.diff(stateseq_padded).nonzero()[0]\n",
    "    return changepoints[1:]-changepoints[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = 10\n",
    "folder = Path(f'/n/groups/datta/win/longtogeny/data/ontogeny/version_{version:02d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mixed_map_gpus(2)\n",
    "set_mixed_map_iters(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_path = folder / '_pca/pca_scores.h5'\n",
    "\n",
    "with h5py.File(pca_path, 'r') as h5f:\n",
    "    pc_data = {k: h5f['scores'][k][:, :10] for k in h5f['scores']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pc_data = whiten_all(pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_threshold = 300  # frames\n",
    "pc_data = valfilter(lambda v: np.isnan(v).any(1).sum() < nan_threshold, pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_sessions = [\n",
    "    \"377cc5ba-e86b-4f80-a218-17f559e5fc2b\",\n",
    "    \"7a5ec9d6-f0ac-403c-a35b-7b12af9918ce\",\n",
    "    \"0c6d9626-c2bf-4615-b044-48674ba1dd16\",\n",
    "    \"3a683c7c-4f28-465a-8abf-783a5219bead\",\n",
    "]\n",
    "pc_data = keyfilter(lambda k: k not in remove_sessions, pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_data = {k: v for i, (k, v) in enumerate(pc_data.items()) if i < 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23023312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_frames = sum(map(len, pc_data.values()))\n",
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_frames = 20e6\n",
    "frames_per_session = int(max_frames // len(pc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39138"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_per_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab the middle of the dataset\n",
    "def slice_data(v):\n",
    "    diff = len(v) - frames_per_session\n",
    "    return v[diff // 2:frames_per_session + diff // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data = valmap(slice_data, pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_states = 100\n",
    "nlags = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "    data[\"x\"], data[\"mask\"], lbls = batch(pc_data)\n",
    "\n",
    "non_nans = ~jnp.isnan(data['x']).any(-1)\n",
    "mask = [jnp.roll(non_nans, shift) for shift in range(nlags + 1)]\n",
    "mask = jnp.all(jnp.array(mask), axis=0)\n",
    "\n",
    "data['mask'] = jnp.where(mask, data['mask'], 0)\n",
    "del mask\n",
    "del non_nans\n",
    "\n",
    "data['x'] = jnp.where(jnp.isnan(data['x']), 0, data['x'])\n",
    "data = convert_data_precision(data, x64=True)\n",
    "data['mask'] = data['mask'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kappas = np.logspace(7.5, 9, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = obs_dim = data['x'].shape[-1]\n",
    "ar_hypparams = {\n",
    "    'S_0_scale': .01,\n",
    "    'K_0_scale': 10,\n",
    "    'num_states': num_states,\n",
    "    'nlags': nlags,\n",
    "    'latent_dim': latent_dim\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_kappa(kappas, num_iters=10):\n",
    "    durations = {}\n",
    "    for k in tqdm(kappas, desc=\"kappa scan\"):\n",
    "        trans_hypparams = {\n",
    "            \"gamma\": 1e3,\n",
    "            \"alpha\": 5.7,\n",
    "            \"kappa\": k,\n",
    "            \"num_states\": num_states,\n",
    "        }\n",
    "\n",
    "        model = arhmm.init_model(\n",
    "            data,\n",
    "            ar_hypparams=ar_hypparams,\n",
    "            trans_hypparams=trans_hypparams,\n",
    "            robust=False,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        pbar = tqdm(range(num_iters))\n",
    "\n",
    "        for i in pbar:\n",
    "            # Perform Gibbs resampling\n",
    "            model = arhmm.resample_model(data, **model, verbose=False)\n",
    "\n",
    "            durs = get_durations(model[\"states\"][\"z\"], data[\"mask\"])\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"Dur {np.mean(durs) / 30:0.2f}s {np.median(durs) / 30:0.2f}s\"\n",
    "            )\n",
    "        durations[k] = (np.mean(durs) / 30, np.median(durs) / 30)\n",
    "\n",
    "    best_duration = valmap(lambda v: np.abs(v[0] - 0.6), durations)\n",
    "    best_kappa = min(best_duration, key=best_duration.get)\n",
    "    return best_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(f\"/n/scratch3/users/w/wg41/moseq-model-checkpoints/version_{version:02d}\")\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "checkpoints = sorted(checkpoint_path.glob(\"model_params*.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "start = 0 if len(checkpoints) == 0 else int(checkpoints[-1].stem.split('_')[-1]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20166f28ce444a3487b0e8948782e2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll_keys = ['z', 'x']\n",
    "ll_history = {key: [] for key in ll_keys}\n",
    "\n",
    "\n",
    "if start == 0:\n",
    "    best_kappa = find_kappa(kappas, num_iters=15)\n",
    "    trans_hypparams = {\n",
    "        'gamma': 1e3,\n",
    "        'alpha': 5.7,\n",
    "        'kappa': best_kappa,\n",
    "        'num_states': num_states\n",
    "    }\n",
    "    model = arhmm.init_model(\n",
    "        data,\n",
    "        ar_hypparams=ar_hypparams,\n",
    "        trans_hypparams=trans_hypparams,\n",
    "        robust=False,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    model = joblib.load(checkpoints[-1])\n",
    "\n",
    "pbar = tqdm(range(start, num_iters))\n",
    "\n",
    "for i in pbar:\n",
    "    # Perform Gibbs resampling\n",
    "    model = arhmm.resample_model(data, **model)\n",
    "    # durs = get_durations(model['states']['z'], data['mask'])\n",
    "\n",
    "    # Compute the likelihood of the data and\n",
    "    # resampled states given the resampled params\n",
    "    # ll = arhmm.model_likelihood(data, **model)\n",
    "    # for key in ll_keys:\n",
    "    #     ll_history[key].append(ll[key].item())\n",
    "    # pbar.set_description(f\"LL: {ll['x']:0.2e} -- Dur {np.mean(durs) / 30:0.2f}s {np.median(durs) / 30:0.2f}s\")\n",
    "    if i % 5 == 0:\n",
    "        joblib.dump(model, checkpoint_path / f\"model_params_{i:03d}.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(model, folder / 'model_params.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-moseq-og",
   "language": "python",
   "name": "jax-moseq-og"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
