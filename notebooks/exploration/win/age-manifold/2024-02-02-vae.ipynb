{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123687f-1a98-448d-883c-4e046216798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2886e-b9e9-497c-b413-07f67a06abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagLinear(nn.Module):\n",
    "    \"\"\"Applies a diagonal linear transformation to the incoming data: :math:`y = xD^T + b`\"\"\"\n",
    "\n",
    "    __constants__ = ['features']\n",
    "    # features: int\n",
    "    # weight: Tensor\n",
    "\n",
    "    def __init__(self, features, bias=True):\n",
    "\n",
    "        super(DiagLinear, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.weight = nn.Parameter(torch.Tensor(features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / np.sqrt(self.features)\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            bound = 1 / np.sqrt(self.features)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.mul(self.weight)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'features={}, bias={}'.format(self.features, self.bias is not None)\n",
    "\n",
    "\n",
    "class ConvAEEncoder(BaseModule):\n",
    "    \"\"\"Convolutional encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hparams : :obj:`dict`\n",
    "            - 'model_class' (:obj:`str`): 'ae' | 'vae'\n",
    "            - 'n_ae_latents' (:obj:`int`)\n",
    "            - 'fit_sess_io_layers; (:obj:`bool`): fit session-specific input/output layers\n",
    "            - 'ae_encoding_x_dim' (:obj:`list`)\n",
    "            - 'ae_encoding_y_dim' (:obj:`list`)\n",
    "            - 'ae_encoding_n_channels' (:obj:`list`)\n",
    "            - 'ae_encoding_kernel_size' (:obj:`list`)\n",
    "            - 'ae_encoding_stride_size' (:obj:`list`)\n",
    "            - 'ae_encoding_x_padding' (:obj:`list`)\n",
    "            - 'ae_encoding_y_padding' (:obj:`list`)\n",
    "            - 'ae_encoding_layer_type' (:obj:`list`)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.encoder = None\n",
    "        self.build_model()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Pretty print encoder architecture.\"\"\"\n",
    "        format_str = 'Encoder architecture:\\n'\n",
    "        i = 0\n",
    "        for module in self.encoder:\n",
    "            format_str += str('    {:02d}: {}\\n'.format(i, module))\n",
    "            i += 1\n",
    "        # final ff layer\n",
    "        format_str += str('    {:02d}: {}\\n'.format(i, self.FF))\n",
    "        return format_str\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Construct the encoder.\"\"\"\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        # Loop over layers (each conv/batch norm/max pool/relu chunk counts as\n",
    "        # one layer for global_layer_num)\n",
    "        global_layer_num = 0\n",
    "        for i_layer in range(0, len(self.hparams['ae_encoding_n_channels'])):\n",
    "\n",
    "            # only add if conv layer (checks within this for max pool layer)\n",
    "            if self.hparams['ae_encoding_layer_type'][i_layer] == 'conv':\n",
    "\n",
    "                # convolution layer\n",
    "                args = self._get_conv2d_args(i_layer, global_layer_num)\n",
    "                if self.hparams.get('fit_sess_io_layers', False) and i_layer == 0:\n",
    "                    module = nn.ModuleList([\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=args['in_channels'],\n",
    "                            out_channels=args['out_channels'],\n",
    "                            kernel_size=args['kernel_size'],\n",
    "                            stride=args['stride'],\n",
    "                            padding=args['padding'])\n",
    "                        for _ in range(self.hparams['n_datasets'])])\n",
    "                    self.encoder.add_module(\n",
    "                        str('conv%i_sess_io_layers' % global_layer_num), module)\n",
    "                else:\n",
    "                    module = nn.Conv2d(\n",
    "                        in_channels=args['in_channels'],\n",
    "                        out_channels=args['out_channels'],\n",
    "                        kernel_size=args['kernel_size'],\n",
    "                        stride=args['stride'],\n",
    "                        padding=args['padding'])\n",
    "                    self.encoder.add_module(\n",
    "                        str('conv%i' % global_layer_num), module)\n",
    "\n",
    "                # batch norm layer\n",
    "                if self.hparams['ae_batch_norm']:\n",
    "                    module = nn.BatchNorm2d(\n",
    "                        self.hparams['ae_encoding_n_channels'][i_layer],\n",
    "                        momentum=self.hparams.get('ae_batch_norm_momentum', 0.1),\n",
    "                        track_running_stats=self.hparams.get('track_running_stats', True))\n",
    "                    self.encoder.add_module(\n",
    "                        str('batchnorm%i' % global_layer_num), module)\n",
    "\n",
    "                # max pool layer\n",
    "                if i_layer < (len(self.hparams['ae_encoding_n_channels'])-1) \\\n",
    "                        and (self.hparams['ae_encoding_layer_type'][i_layer+1] == 'maxpool'):\n",
    "                    args = self._get_maxpool2d_args(i_layer)\n",
    "                    module = nn.MaxPool2d(\n",
    "                        kernel_size=args['kernel_size'],\n",
    "                        stride=args['stride'],\n",
    "                        padding=args['padding'],\n",
    "                        return_indices=args['return_indices'],\n",
    "                        ceil_mode=args['ceil_mode'])\n",
    "                    self.encoder.add_module(\n",
    "                        str('maxpool%i' % global_layer_num), module)\n",
    "\n",
    "                # leaky ReLU\n",
    "                self.encoder.add_module(\n",
    "                    str('relu%i' % global_layer_num), nn.LeakyReLU(0.05))\n",
    "                global_layer_num += 1\n",
    "\n",
    "        # final ff layer to latents\n",
    "        last_conv_size = self.hparams['ae_encoding_n_channels'][-1] \\\n",
    "            * self.hparams['ae_encoding_y_dim'][-1] \\\n",
    "            * self.hparams['ae_encoding_x_dim'][-1]\n",
    "        self.FF = nn.Linear(last_conv_size, self.hparams['n_ae_latents'])\n",
    "\n",
    "        # If VAE model, have additional ff layer to latent variances\n",
    "        if self.hparams.get('variational', False):\n",
    "            self.logvar = nn.Linear(last_conv_size, self.hparams['n_ae_latents'])\n",
    "\n",
    "    def _get_conv2d_args(self, layer, global_layer):\n",
    "\n",
    "        if layer == 0:\n",
    "            if self.hparams['model_class'] == 'cond-ae' and \\\n",
    "                    self.hparams.get('conditional_encoder', False):\n",
    "                # labels will be appended to input if using conditional autoencoder with\n",
    "                # conditional encoder flag\n",
    "                n_labels = int(self.hparams['n_labels'] / 2)  # 'n_labels' key includes x/y coords\n",
    "            else:\n",
    "                n_labels = 0\n",
    "            in_channels = self.hparams['ae_input_dim'][0] + n_labels\n",
    "        else:\n",
    "            in_channels = self.hparams['ae_encoding_n_channels'][layer - 1]\n",
    "\n",
    "        out_channels = self.hparams['ae_encoding_n_channels'][layer]\n",
    "        kernel_size = self.hparams['ae_encoding_kernel_size'][layer]\n",
    "        stride = self.hparams['ae_encoding_stride_size'][layer]\n",
    "\n",
    "        x_pad_0 = self.hparams['ae_encoding_x_padding'][layer][0]\n",
    "        x_pad_1 = self.hparams['ae_encoding_x_padding'][layer][1]\n",
    "        y_pad_0 = self.hparams['ae_encoding_y_padding'][layer][0]\n",
    "        y_pad_1 = self.hparams['ae_encoding_y_padding'][layer][1]\n",
    "        if (x_pad_0 == x_pad_1) and (y_pad_0 == y_pad_1):\n",
    "            # if symmetric padding\n",
    "            padding = (y_pad_0, x_pad_0)\n",
    "        else:\n",
    "            module = nn.ZeroPad2d((x_pad_0, x_pad_1, y_pad_0, y_pad_1))\n",
    "            self.encoder.add_module(str('zero_pad%i' % global_layer), module)\n",
    "            padding = 0\n",
    "\n",
    "        args = {\n",
    "            'in_channels': in_channels,\n",
    "            'out_channels': out_channels,\n",
    "            'kernel_size': kernel_size,\n",
    "            'stride': stride,\n",
    "            'padding': padding}\n",
    "        return args\n",
    "\n",
    "    def _get_maxpool2d_args(self, layer):\n",
    "        args = {\n",
    "            'kernel_size': int(self.hparams['ae_encoding_kernel_size'][layer + 1]),\n",
    "            'stride': int(self.hparams['ae_encoding_stride_size'][layer + 1]),\n",
    "            'padding': (\n",
    "                self.hparams['ae_encoding_y_padding'][layer + 1][0],\n",
    "                self.hparams['ae_encoding_x_padding'][layer + 1][0]),\n",
    "            'return_indices': True}\n",
    "        if self.hparams['ae_padding_type'] == 'valid':\n",
    "            # no ceil mode in valid mode\n",
    "            args['ceil_mode'] = False\n",
    "        else:\n",
    "            # using ceil mode instead of zero padding\n",
    "            args['ceil_mode'] = True\n",
    "        return args\n",
    "\n",
    "    def forward(self, x, dataset=None):\n",
    "        \"\"\"Process input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : :obj:`torch.Tensor` object\n",
    "            input data\n",
    "        dataset : :obj:`int`\n",
    "            used with session-specific io layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :obj:`tuple`\n",
    "            - encoder output (:obj:`torch.Tensor`): shape (n_latents)\n",
    "            - pool_idx (:obj:`list`): max pooling indices for each layer\n",
    "            - output_size (:obj:`list`): output size for each layer\n",
    "\n",
    "        \"\"\"\n",
    "        # loop over layers, have to collect pool_idx and output sizes if using max pooling to use\n",
    "        # in unpooling\n",
    "        pool_idx = []\n",
    "        target_output_size = []\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                target_output_size.append(x.size())\n",
    "                x, idx = layer(x)\n",
    "                pool_idx.append(idx)\n",
    "            elif isinstance(layer, nn.ModuleList):\n",
    "                x = layer[dataset](x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        # reshape for ff layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.hparams.get('variational', False):\n",
    "            return self.FF(x), self.logvar(x), pool_idx, target_output_size\n",
    "        else:\n",
    "            return self.FF(x), pool_idx, target_output_size\n",
    "\n",
    "\n",
    "class AEPSEncoder(ConvAEEncoder):\n",
    "    \"\"\"Encoder that separates label-related subspace.\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "\n",
    "        from behavenet.models.base import DiagLinear\n",
    "\n",
    "        super().__init__(hparams)\n",
    "\n",
    "        # add linear transformations mapping from NN output to label-, non-label-related subspaces\n",
    "        n_latents = self.hparams['n_ae_latents']\n",
    "        n_labels = self.hparams['n_labels']\n",
    "        # NN -> constrained latents\n",
    "        self.A = nn.Linear(n_latents, n_labels, bias=False)\n",
    "        # NN -> unconstrained latents\n",
    "        self.B = nn.Linear(n_latents, n_latents - n_labels, bias=False)\n",
    "        # constrained latents -> labels (diagonal matrix + bias)\n",
    "        self.D = DiagLinear(n_labels, bias=True)\n",
    "\n",
    "        # fix A, B to be orthogonal (and not trainable)\n",
    "        from scipy.stats import ortho_group\n",
    "        m = ortho_group.rvs(dim=n_latents).astype('float32')\n",
    "        with torch.no_grad():\n",
    "            self.A.weight = nn.Parameter(\n",
    "                torch.from_numpy(m[:n_labels, :]), requires_grad=False)\n",
    "            self.B.weight = nn.Parameter(\n",
    "                torch.from_numpy(m[n_labels:, :]), requires_grad=False)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Pretty print encoder architecture.\"\"\"\n",
    "        format_str = 'Encoder architecture:\\n'\n",
    "        i = 0\n",
    "        for module in self.encoder:\n",
    "            format_str += str('    {:02d}: {}\\n'.format(i, module))\n",
    "            i += 1\n",
    "        # final ff layer\n",
    "        format_str += str('    {:02d}: {}\\n'.format(i, self.FF))\n",
    "        # final linear transformations\n",
    "        format_str += str('    {:02d}: {} (to constrained latents)\\n'.format(i, self.A))\n",
    "        format_str += str('    {:02d}: {} (to unconstrained latents)\\n'.format(i, self.B))\n",
    "        format_str += str('    {:02d}: {} (constrained latents to labels)\\n'.format(i, self.D))\n",
    "        return format_str\n",
    "\n",
    "    def forward(self, x, dataset=None):\n",
    "        \"\"\"Process input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : :obj:`torch.Tensor` object\n",
    "            input data\n",
    "        dataset : :obj:`int`\n",
    "            used with session-specific io layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :obj:`tuple`\n",
    "            - encoder output y (:obj:`torch.Tensor`): constrained latents (predicted labels) of\n",
    "              shape (n_labels)\n",
    "            - encoder output z (:obj:`torch.Tensor`): unconstrained latents of shape\n",
    "              (n_latents - n_labels)\n",
    "            - logvar (:obj:`torch.Tensor`): log variance of latents of shape (n_latents)\n",
    "            - pool_idx (:obj:`list`): max pooling indices for each layer\n",
    "            - output_size (:obj:`list`): output size for each layer\n",
    "\n",
    "        \"\"\"\n",
    "        # loop over layers, have to collect pool_idx and output sizes if using max pooling to use\n",
    "        # in unpooling\n",
    "        pool_idx = []\n",
    "        target_output_size = []\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                target_output_size.append(x.size())\n",
    "                x, idx = layer(x)\n",
    "                pool_idx.append(idx)\n",
    "            elif isinstance(layer, nn.ModuleList):\n",
    "                x = layer[dataset](x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        # reshape for ff layer\n",
    "        x1 = x.view(x.size(0), -1)\n",
    "        x = self.FF(x1)\n",
    "\n",
    "        # push through linear transformations\n",
    "        y = self.A(x)  # NN -> constrained latents\n",
    "        w = self.B(x)  # NN -> unconstrained latents\n",
    "\n",
    "        return y, w, self.logvar(x1), pool_idx, target_output_size\n",
    "\n",
    "class PSVAE(AE):\n",
    "    \"\"\"Partitioned subspace variational autoencoder class.\n",
    "\n",
    "    This class constructs a VAE that...\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"See constructor documentation of AE for hparams details.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hparams : :obj:`dict`\n",
    "            in addition to the standard keys, must also contain:\n",
    "            - 'n_labels' (:obj:`n_labels`)\n",
    "            - 'ps_vae.alpha' (:obj:`float`)\n",
    "            - 'ps_vae.beta' (:obj:`float`)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if hparams['model_type'] == 'linear':\n",
    "            raise NotImplementedError\n",
    "        if hparams['n_ae_latents'] < hparams['n_labels']:\n",
    "            raise ValueError('PS-VAE model must contain at least as many latents as labels')\n",
    "\n",
    "        self.n_latents = hparams['n_ae_latents']\n",
    "        self.n_labels = hparams['n_labels']\n",
    "\n",
    "        hparams['variational'] = True\n",
    "        super().__init__(hparams)\n",
    "\n",
    "        # set up beta annealing\n",
    "        anneal_epochs = self.hparams.get('ps_vae.anneal_epochs', 0)\n",
    "        self.curr_epoch = 0  # must be modified by training script\n",
    "        beta = hparams['ps_vae.beta']\n",
    "        # TODO: these values should not be precomputed\n",
    "        if anneal_epochs > 0:\n",
    "            # annealing for total correlation term\n",
    "            self.beta_vals = np.append(\n",
    "                np.linspace(0, beta, anneal_epochs),  # USED TO START AT 1!!\n",
    "                beta * np.ones(hparams['max_n_epochs'] + 1))  # sloppy addition to fully cover rest\n",
    "            # annealing for remaining kl terms - index code mutual info and dim-wise kl\n",
    "            self.kl_anneal_vals = np.append(\n",
    "                np.linspace(0, 1, anneal_epochs),\n",
    "                np.ones(hparams['max_n_epochs'] + 1))  # sloppy addition to fully cover rest\n",
    "        else:\n",
    "            self.beta_vals = beta * np.ones(hparams['max_n_epochs'] + 1)\n",
    "            self.kl_anneal_vals = np.ones(hparams['max_n_epochs'] + 1)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Construct the model using hparams.\"\"\"\n",
    "        self.hparams['hidden_layer_size'] = self.hparams['n_ae_latents']\n",
    "        if self.model_type == 'conv':\n",
    "            self.encoding = ConvAEPSEncoder(self.hparams)\n",
    "            self.decoding = ConvAEDecoder(self.hparams)\n",
    "        elif self.model_type == 'linear':\n",
    "            raise NotImplementedError\n",
    "            # if self.hparams.get('fit_sess_io_layers', False):\n",
    "            #     raise NotImplementedError\n",
    "            # n_latents = self.hparams['n_ae_latents']\n",
    "            # self.encoding = LinearAEEncoder(n_latents, self.img_size)\n",
    "            # self.decoding = LinearAEDecoder(n_latents, self.img_size, self.encoding)\n",
    "        else:\n",
    "            raise ValueError('\"%s\" is an invalid model_type' % self.model_type)\n",
    "\n",
    "    def forward(self, x, dataset=None, use_mean=False, **kwargs):\n",
    "        \"\"\"Process input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : :obj:`torch.Tensor` object\n",
    "            input data of shape (n_frames, n_channels, y_pix, x_pix)\n",
    "        dataset : :obj:`int`\n",
    "            used with session-specific io layers\n",
    "        use_mean : :obj:`bool`\n",
    "            True to skip sampling step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :obj:`tuple`\n",
    "            - x_hat (:obj:`torch.Tensor`): output of shape (n_frames, n_channels, y_pix, x_pix)\n",
    "            - z (:obj:`torch.Tensor`): sampled latent variable of shape (n_frames, n_latents)\n",
    "            - mu (:obj:`torch.Tensor`): mean paramter of shape (n_frames, n_latents)\n",
    "            - logvar (:obj:`torch.Tensor`): logvar paramter of shape (n_frames, n_latents)\n",
    "            - y_hat (:obj:`torch.Tensor`): output of shape (n_frames, n_labels)\n",
    "\n",
    "        \"\"\"\n",
    "        y, w, logvar, pool_idx, outsize = self.encoding(x, dataset=dataset)\n",
    "        mu = torch.cat([y, w], axis=1)\n",
    "        if use_mean:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = reparameterize(mu, logvar)\n",
    "        x_hat = self.decoding(z, pool_idx, outsize, dataset=dataset)\n",
    "        y_hat = self.encoding.D(y)\n",
    "        return x_hat, z, mu, logvar, y_hat\n",
    "\n",
    "    def loss(self, data, dataset=0, accumulate_grad=True, chunk_size=200):\n",
    "        \"\"\"Calculate modified ELBO loss for PSVAE.\n",
    "\n",
    "        The batch is split into chunks if larger than a hard-coded `chunk_size` to keep memory\n",
    "        requirements low; gradients are accumulated across all chunks before a gradient step is\n",
    "        taken.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : :obj:`dict`\n",
    "            batch of data; keys should include 'images' and 'masks', if necessary\n",
    "        dataset : :obj:`int`, optional\n",
    "            used for session-specific io layers\n",
    "        accumulate_grad : :obj:`bool`, optional\n",
    "            accumulate gradient for training step\n",
    "        chunk_size : :obj:`int`, optional\n",
    "            batch is split into chunks of this size to keep memory requirements low\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :obj:`dict`\n",
    "            - 'loss' (:obj:`float`): full elbo\n",
    "            - 'loss_ll' (:obj:`float`): log-likelihood portion of elbo\n",
    "            - 'loss_kl' (:obj:`float`): kl portion of elbo\n",
    "            - 'loss_mse' (:obj:`float`): mse (without gaussian constants)\n",
    "            - 'beta' (:obj:`float`): weight in front of kl term\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = data['images'][0]\n",
    "        y = data['labels'][0]\n",
    "        m = data['masks'][0] if 'masks' in data else None\n",
    "        n = data['labels_masks'][0] if 'labels_masks' in data else None\n",
    "        batch_size = x.shape[0]\n",
    "        n_chunks = int(np.ceil(batch_size / chunk_size))\n",
    "        n_labels = self.hparams['n_labels']\n",
    "        # n_latents = self.hparams['n_ae_latents']\n",
    "\n",
    "        # compute hyperparameters\n",
    "        alpha = self.hparams['ps_vae.alpha']\n",
    "        beta = self.beta_vals[self.curr_epoch]\n",
    "        kl = self.kl_anneal_vals[self.curr_epoch]\n",
    "\n",
    "        loss_strs = [\n",
    "            'loss', 'loss_data_ll', 'loss_label_ll', 'loss_zs_kl', 'loss_zu_mi', 'loss_zu_tc',\n",
    "            'loss_zu_dwkl']\n",
    "\n",
    "        loss_dict_vals = {loss: 0 for loss in loss_strs}\n",
    "        loss_dict_vals['loss_data_mse'] = 0\n",
    "\n",
    "        y_hat_all = []\n",
    "\n",
    "        for chunk in range(n_chunks):\n",
    "\n",
    "            idx_beg = chunk * chunk_size\n",
    "            idx_end = np.min([(chunk + 1) * chunk_size, batch_size])\n",
    "\n",
    "            x_in = x[idx_beg:idx_end]\n",
    "            y_in = y[idx_beg:idx_end]\n",
    "            m_in = m[idx_beg:idx_end] if m is not None else None\n",
    "            n_in = n[idx_beg:idx_end] if n is not None else None\n",
    "            x_hat, sample, mu, logvar, y_hat = self.forward(x_in, dataset=dataset, use_mean=False)\n",
    "\n",
    "            # reset losses\n",
    "            loss_dict_torch = {loss: 0 for loss in loss_strs}\n",
    "\n",
    "            # data log-likelihood\n",
    "            loss_dict_torch['loss_data_ll'] = losses.gaussian_ll(x_in, x_hat, m_in)\n",
    "            loss_dict_torch['loss'] -= loss_dict_torch['loss_data_ll']\n",
    "\n",
    "            # label log-likelihood\n",
    "            loss_dict_torch['loss_label_ll'] = losses.gaussian_ll(y_in, y_hat, n_in)\n",
    "            loss_dict_torch['loss'] -= alpha * loss_dict_torch['loss_label_ll']\n",
    "\n",
    "            # supervised latents kl\n",
    "            loss_dict_torch['loss_zs_kl'] = losses.kl_div_to_std_normal(\n",
    "                mu[:, :n_labels], logvar[:, :n_labels])\n",
    "            loss_dict_torch['loss'] += loss_dict_torch['loss_zs_kl']\n",
    "\n",
    "            # compute all terms of decomposed elbo at once\n",
    "            index_code_mi, total_correlation, dimension_wise_kl = losses.decomposed_kl(\n",
    "                sample[:, n_labels:], mu[:, n_labels:], logvar[:, n_labels:])\n",
    "\n",
    "            # unsupervised latents index-code mutual information\n",
    "            loss_dict_torch['loss_zu_mi'] = index_code_mi\n",
    "            loss_dict_torch['loss'] += kl * loss_dict_torch['loss_zu_mi']\n",
    "\n",
    "            # unsupervised latents total correlation\n",
    "            loss_dict_torch['loss_zu_tc'] = total_correlation\n",
    "            loss_dict_torch['loss'] += beta * loss_dict_torch['loss_zu_tc']\n",
    "\n",
    "            # unsupervised latents dimension-wise kl\n",
    "            loss_dict_torch['loss_zu_dwkl'] = dimension_wise_kl\n",
    "            loss_dict_torch['loss'] += kl * loss_dict_torch['loss_zu_dwkl']\n",
    "\n",
    "            if accumulate_grad:\n",
    "                loss_dict_torch['loss'].backward()\n",
    "\n",
    "            # get loss value (weighted by batch size)\n",
    "            bs = idx_end - idx_beg\n",
    "            for key, val in loss_dict_torch.items():\n",
    "                loss_dict_vals[key] += val.item() * bs\n",
    "            loss_dict_vals['loss_data_mse'] += losses.gaussian_ll_to_mse(\n",
    "                loss_dict_vals['loss_data_ll'] / bs, np.prod(x.shape[1:])) * bs\n",
    "\n",
    "            # collect predicted labels to compute R2\n",
    "            y_hat_all.append(y_hat.cpu().detach().numpy())\n",
    "\n",
    "        # use variance-weighted r2s to ignore small-variance latents\n",
    "        y_hat_all = np.concatenate(y_hat_all, axis=0)\n",
    "        y_all = y.cpu().detach().numpy()\n",
    "        if n is not None:\n",
    "            n_np = n.cpu().detach().numpy()\n",
    "            r2 = r2_score(y_all[n_np == 1], y_hat_all[n_np == 1], multioutput='variance_weighted')\n",
    "        else:\n",
    "            r2 = r2_score(y_all, y_hat_all, multioutput='variance_weighted')\n",
    "\n",
    "        # compile (properly weighted) loss terms\n",
    "        for key in loss_dict_vals.keys():\n",
    "            loss_dict_vals[key] /= batch_size\n",
    "\n",
    "        # store hyperparams\n",
    "        loss_dict_vals['alpha'] = alpha\n",
    "        loss_dict_vals['beta'] = beta\n",
    "        loss_dict_vals['label_r2'] = r2\n",
    "\n",
    "        return loss_dict_vals\n",
    "\n",
    "    def get_predicted_labels(self, x, dataset=None, use_mean=True):\n",
    "        \"\"\"Process input data to get predicted labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : :obj:`torch.Tensor` object\n",
    "            input data of shape (n_frames, n_channels, y_pix, x_pix)\n",
    "        dataset : :obj:`int`\n",
    "            used with session-specific io layers\n",
    "        use_mean : :obj:`bool`\n",
    "            True to skip sampling step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :obj:`torch.Tensor`\n",
    "            output of shape (n_frames, n_labels)\n",
    "\n",
    "        \"\"\"\n",
    "        y, w, logvar, pool_idx, outsize = self.encoding(x, dataset=dataset)\n",
    "        if not use_mean:\n",
    "            y = reparameterize(y, logvar[:, :self.n_labels])\n",
    "        y_hat = self.encoding.D(y)\n",
    "        return y_hat\n",
    "\n",
    "    def get_transformed_latents(self, inputs, dataset=None, as_numpy=True):\n",
    "        \"\"\"Return latents after supervised subspace has been transformed to original label space.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : :obj:`torch.Tensor` object\n",
    "            - image tensor of shape (n_frames, n_channels, y_pix, x_pix)\n",
    "            - latents tensor of shape (n_frames, n_ae_latents)\n",
    "        dataset : :obj:`int`, optional\n",
    "            used with session-specific io layers\n",
    "        as_numpy : :obj:`bool`, optional\n",
    "            True to return as numpy array, False to return as torch Tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :obj:`np.ndarray` or :obj:`torch.Tensor` object\n",
    "            array of latents in transformed latent space of shape (n_frames, n_latents)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.Tensor(inputs)\n",
    "\n",
    "        # check to see if inputs are images or latents\n",
    "        if len(inputs.shape) == 2:\n",
    "            input_type = 'latents'\n",
    "        else:\n",
    "            input_type = 'images'\n",
    "\n",
    "        # get latents in original space\n",
    "        if input_type == 'images':\n",
    "            y_og, w_og, logvar, pool_idx, outsize = self.encoding(inputs, dataset=dataset)\n",
    "        else:\n",
    "            y_og = inputs[:, :self.hparams['n_labels']]\n",
    "            w_og = inputs[:, self.hparams['n_labels']:]\n",
    "\n",
    "        # transform supervised latents to label space\n",
    "        y_new = self.encoding.D(y_og)\n",
    "\n",
    "        latents_tr = torch.cat([y_new, w_og], axis=1)\n",
    "\n",
    "        if as_numpy:\n",
    "            return latents_tr.cpu().detach().numpy()\n",
    "        else:\n",
    "            return latents_tr\n",
    "\n",
    "    def get_inverse_transformed_latents(self, inputs, dataset=None, as_numpy=True):\n",
    "        \"\"\"Return latents after they have been transformed using the diagonal mapping D.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : :obj:`torch.Tensor` object\n",
    "            - image tensor of shape (n_frames, n_channels, y_pix, x_pix)\n",
    "            - latents tensor of shape (n_frames, n_ae_latents) where the first n_labels entries are\n",
    "              assumed to be labels in the original pixel space\n",
    "        dataset : :obj:`int`, optional\n",
    "            used with session-specific io layers\n",
    "        as_numpy : :obj:`bool`, optional\n",
    "            True to return as numpy array, False to return as torch Tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :obj:`np.ndarray` or :obj:`torch.Tensor` object\n",
    "            array of latents in transformed latent space of shape (n_frames, n_latents)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.Tensor(inputs)\n",
    "\n",
    "        # check to see if inputs are images or latents\n",
    "        if len(inputs.shape) == 2:\n",
    "            input_type = 'latents'\n",
    "        else:\n",
    "            input_type = 'images'\n",
    "\n",
    "        # get latents in original space\n",
    "        if input_type == 'images':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            y_og = inputs[:, :self.hparams['n_labels']]\n",
    "            w_og = inputs[:, self.hparams['n_labels']:]\n",
    "\n",
    "        # transform given labels to latent space\n",
    "        y_new = torch.div(torch.sub(y_og, self.encoding.D.bias), self.encoding.D.weight)\n",
    "\n",
    "        latents_tr = torch.cat([y_new, w_og], axis=1)\n",
    "\n",
    "        if as_numpy:\n",
    "            return latents_tr.cpu().detach().numpy()\n",
    "        else:\n",
    "            return latents_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a3b6be-a2ed-4dd8-89fd-c7bb37ec9e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgeManifoldVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        activation,\n",
    "        hidden_dims: list | tuple,\n",
    "        latent_dim,\n",
    "        mdl_type=\"regressor\",\n",
    "        n_classes=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.n_layers = len(hidden_dims)\n",
    "        self.hidden_dim = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        if n_classes is None:\n",
    "            n_classes = latent_dim\n",
    "\n",
    "        layers = []\n",
    "        for _in, _out in zip(hidden_dims, hidden_dims[1:]):\n",
    "            layers.append(nn.Linear(_in, _out))\n",
    "            layers.append(nn.BatchNorm1d(_out))\n",
    "            layers.append(activation())\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dims[0]),\n",
    "            activation(),\n",
    "            *layers,\n",
    "            nn.Linear(hidden_dims[-1], latent_dim),\n",
    "            # activation(),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        for _in, _out in zip(hidden_dims[::-1], hidden_dims[::-1][1:]):\n",
    "            layers.append(nn.Linear(_in, _out))\n",
    "            layers.append(nn.BatchNorm1d(_out))\n",
    "            layers.append(activation())\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dims[-1]),\n",
    "            activation(),\n",
    "            *layers,\n",
    "            nn.Linear(hidden_dims[0], input_dims),\n",
    "        )\n",
    "        # linear regressor to predict age\n",
    "        if mdl_type == \"regressor\":\n",
    "            self.lm = nn.Linear(latent_dim, 1)\n",
    "        else:\n",
    "            self.lm = nn.Linear(latent_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.transform(x)\n",
    "        return self.decoder(latent), self.lm(latent), latent\n",
    "\n",
    "    def transform(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.lm(self.transform(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ontogeny",
   "language": "python",
   "name": "aging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
