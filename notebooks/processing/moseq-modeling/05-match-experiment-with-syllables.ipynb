{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da727b1e-e418-443c-9fce-881bd4ba6863",
   "metadata": {},
   "source": [
    "# Take inferred syllables from ARHMM and assign mouse ID and experiment to each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa78d85-33c0-4298-a61e-1d32c52ff5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from toolz import concat, curry, compose, valmap, first, groupby\n",
    "from toolz.curried import pluck\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95056800-ad02-4ab1-8960-55059a98ef1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folders = [\n",
    "    '/n/groups/datta/Dana/Ontogeny/raw_data/Ontogeny_females',\n",
    "    '/n/groups/datta/Dana/Ontogeny/raw_data/Ontogeny_males',\n",
    "    '/n/groups/datta/Dana/Ontogeny/raw_data/longtogeny_pre_unet/Females',\n",
    "    '/n/groups/datta/Dana/Ontogeny/raw_data/longtogeny_pre_unet/Males',\n",
    "    # '/n/groups/datta/min/dominance_v1',\n",
    "    # '/n/groups/datta/min/community_v1',\n",
    "    # '/n/groups/datta/min/wheel_062023',\n",
    "    # '/n/groups/datta/min/cas_behavior_01',\n",
    "]\n",
    "folders = [Path(f) for f in folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be492ef5-ef8e-4d7c-9fdd-d72baca35640",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679b8b7af18844198f727ace8ad43611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uuid_map = {}\n",
    "for file in tqdm(concat(f.glob('**/results_00.h5') for f in folders)):\n",
    "    with h5py.File(file, 'r') as h5f:\n",
    "        uuid = h5f['metadata/uuid'][()].decode()\n",
    "        uuid_map[uuid] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db9d2bc-0ce4-407f-bc80-4a3ba4546a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = Path('/n/groups/datta/win/longtogeny/data/ontogeny/version_03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df45a5bb-46a4-4e67-8093-ddcb2f455e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syllable_path = data_folder / 'all_data_pca/syllables.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a3e800-4315-4b0f-9cea-0c68419bf18a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ColorDataType', 'ColorResolution', 'DepthDataType', 'DepthResolution', 'IsLittleEndian', 'NidaqChannels', 'NidaqSamplingRate', 'SessionName', 'StartTime', 'SubjectName']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(first(uuid_map.values()), 'r') as h5f:\n",
    "    print(list(h5f['metadata/acquisition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a40f9a7-9398-4e24-b727-4a9936c33799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: fill out data organization better here:\n",
    "experiments = (\n",
    "    'ontogeny_males',\n",
    "    'ontogeny_females',\n",
    "    'longtogeny_females',\n",
    "    'longtogeny_males',\n",
    "    # others...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f702e8b-e36a-4cbb-b5f4-9bb180dd7c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_experiment(path: Path):\n",
    "    if \"longtogeny\" in str(path):\n",
    "        sex = path.parents[3].name.lower()\n",
    "        if sex not in (\"males\", \"females\"):\n",
    "            sex = path.parents[2].name.lower()\n",
    "            if sex not in (\"males\", \"females\"):\n",
    "                raise ValueError(\"bleh\")\n",
    "        exp = f\"longtogeny_{sex}\"\n",
    "    elif \"ontogeny\" in str(path).lower() and \"community\" not in str(path):\n",
    "        exp = path.parents[3].name.lower()\n",
    "    else:\n",
    "        exp = path.parents[2].name\n",
    "    return exp\n",
    "\n",
    "\n",
    "@curry\n",
    "def is_experiment(experiment, uuid):\n",
    "    return experiment == get_experiment(uuid_map[uuid])\n",
    "\n",
    "\n",
    "def insert_nans(timestamps, data, fps=30):\n",
    "    df_timestamps = np.diff(np.insert(timestamps, 0, timestamps[0] - 1.0 / fps))\n",
    "    missing_frames = np.floor(df_timestamps / (1.0 / fps))\n",
    "\n",
    "    fill_idx = np.where(missing_frames > 1)[0]\n",
    "    data_idx = np.arange(len(timestamps)).astype('float64')\n",
    "\n",
    "    filled_data = deepcopy(data)\n",
    "    filled_timestamps = deepcopy(timestamps)\n",
    "\n",
    "    if filled_data.ndim == 1:\n",
    "        isvec = True\n",
    "        filled_data = filled_data[:, None]\n",
    "    else:\n",
    "        isvec = False\n",
    "    nframes, nfeatures = filled_data.shape\n",
    "\n",
    "    for idx in fill_idx[::-1]:\n",
    "        if idx < len(missing_frames):\n",
    "            ninserts = int(missing_frames[idx] - 1)\n",
    "            data_idx = np.insert(data_idx, idx, [np.nan] * ninserts)\n",
    "            insert_timestamps = timestamps[idx - 1] + \\\n",
    "                np.cumsum(np.ones(ninserts,) * 1.0 / fps)\n",
    "            filled_data = np.insert(filled_data, idx,\n",
    "                                    np.ones((ninserts, nfeatures)) * np.nan, axis=0)\n",
    "            filled_timestamps = np.insert(\n",
    "                filled_timestamps, idx, insert_timestamps)\n",
    "\n",
    "    if isvec:\n",
    "        filled_data = np.squeeze(filled_data)\n",
    "\n",
    "    return filled_data, data_idx, filled_timestamps\n",
    "\n",
    "\n",
    "def im_moment_features(frame):\n",
    "    frame_mask = frame > 10\n",
    "    cnts, _ = cv2.findContours(frame_mask.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    tmp = np.array([cv2.contourArea(x) for x in cnts])\n",
    "    if tmp.size == 0:\n",
    "        return None\n",
    "    mouse_cnt = cnts[tmp.argmax()]\n",
    "    tmp = cv2.moments(mouse_cnt)\n",
    "    num = 2*tmp['mu11']\n",
    "    den = tmp['mu20']-tmp['mu02']\n",
    "\n",
    "    common = np.sqrt(4*np.square(tmp['mu11'])+np.square(den))\n",
    "\n",
    "    if tmp['m00'] == 0:\n",
    "        features = {\n",
    "            'orientation': np.nan,\n",
    "            'centroid': np.nan,\n",
    "            'axis_length': [np.nan, np.nan]}\n",
    "    else:\n",
    "        features = {\n",
    "            'orientation': -.5*np.arctan2(num, den),\n",
    "            'centroid': [tmp['m10']/tmp['m00'], tmp['m01']/tmp['m00']],\n",
    "            'axis_length': [2*np.sqrt(2)*np.sqrt((tmp['mu20']+tmp['mu02']+common)/tmp['m00']),\n",
    "                            2*np.sqrt(2)*np.sqrt((tmp['mu20']+tmp['mu02']-common)/tmp['m00'])]\n",
    "        }\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def pxs_to_mm(coords, resolution=(512, 424), field_of_view=(70.6, 60), true_depth=673.1):\n",
    "\n",
    "    cx = resolution[0] // 2\n",
    "    cy = resolution[1] // 2\n",
    "\n",
    "    xhat = coords[:, 0] - cx\n",
    "    yhat = coords[:, 1] - cy\n",
    "\n",
    "    fw = resolution[0] / (2 * np.deg2rad(field_of_view[0] / 2))\n",
    "    fh = resolution[1] / (2 * np.deg2rad(field_of_view[1] / 2))\n",
    "\n",
    "    new_coords = np.zeros_like(coords)\n",
    "    new_coords[:, 0] = true_depth * xhat / fw\n",
    "    new_coords[:, 1] = true_depth * yhat / fh\n",
    "\n",
    "    return new_coords\n",
    "\n",
    "\n",
    "def compute_scalars(frames, centroid, true_depth):\n",
    "    centroid_mm = pxs_to_mm(centroid, true_depth=true_depth)\n",
    "    centroid_mm_shift = pxs_to_mm(centroid + 1, true_depth=true_depth)\n",
    "    px_to_mm = np.abs(centroid_mm_shift - centroid_mm)\n",
    "\n",
    "    width = []\n",
    "    length = []\n",
    "    height = []\n",
    "    area = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        # compute ellipse\n",
    "        feats = im_moment_features(frame)\n",
    "        if feats is None:\n",
    "            width.append(np.nan)\n",
    "            length.append(np.nan)\n",
    "            height.append(np.nan)\n",
    "        else:\n",
    "            w = np.min(feats['axis_length'])\n",
    "            w = w * px_to_mm[i, 1]\n",
    "\n",
    "            l = np.max(feats['axis_length'])\n",
    "            l = l * px_to_mm[i, 0]\n",
    "\n",
    "            width.append(w)\n",
    "            length.append(l)\n",
    "            height.append(np.mean(frame[(frame > 10) & (frame < 110)]))\n",
    "        area.append(np.sum((frame > 10) & (frame < 110)) * px_to_mm[i].mean())\n",
    "    out = dict(recon_width=width, recon_length=length, recon_height=height, recon_area=area)\n",
    "    return valmap(np.array, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2016e060-8f4b-4686-a4b6-e126fe8a412a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group uuids by experiment\n",
    "exp_groups = groupby(lambda k: get_experiment(k[1]), uuid_map.items())\n",
    "exp_groups = valmap(compose(list, pluck(0)), exp_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b94c30-03c7-4ee9-ac6f-3ed2e79ca3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c16e1366b9d4cfbb46186a2c1b67d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtogeny_males length 37649629\n"
     ]
    }
   ],
   "source": [
    "recon_frames_key = \"win_size_norm_frames_v2\"\n",
    "failed_sessions = []\n",
    "# batch_size = 200\n",
    "with h5py.File(syllable_path, \"r\") as h5f:\n",
    "    for experiment, uuids in exp_groups.items():\n",
    "        if experiment != 'longtogeny_males':\n",
    "            continue\n",
    "        df = []\n",
    "        for i, (uuid, path) in enumerate(\n",
    "            map(lambda u: (u, uuid_map[u]), tqdm(uuids))\n",
    "        ):\n",
    "            try:\n",
    "                with h5py.File(path, \"r\") as f:\n",
    "                    session_name = f[\"metadata/acquisition/SessionName\"][()].decode()\n",
    "                    subject_name = f[\"metadata/acquisition/SubjectName\"][()].decode()\n",
    "                    keep_scalars = list(filter(lambda k: \"mm\" in k, f[\"scalars\"])) + [\n",
    "                        \"angle\",\n",
    "                        \"velocity_theta\",\n",
    "                    ]\n",
    "\n",
    "                    ts = f[\"timestamps\"][()] / 1000\n",
    "                    scalars = dict((k, f[\"scalars\"][k][()]) for k in keep_scalars)\n",
    "                    filled_scalars = valmap(lambda v: insert_nans(ts, v)[0], scalars)\n",
    "                    filled_ts = insert_nans(ts, ts)[2]\n",
    "\n",
    "                    frames = f[recon_frames_key][()]\n",
    "                    centroid = np.array(\n",
    "                        [f[\"scalars/centroid_x_px\"][()], f[\"scalars/centroid_y_px\"][()]]\n",
    "                    ).T\n",
    "                    true_depth = f[\"metadata/extraction/true_depth\"][()]\n",
    "                    recon_scalars = compute_scalars(frames, centroid, true_depth)\n",
    "                    recon_scalars = valmap(\n",
    "                        lambda v: insert_nans(ts, v)[0], recon_scalars\n",
    "                    )\n",
    "            except KeyError:\n",
    "                session_name = \"\"\n",
    "                subject_name = \"\"\n",
    "            if \"longtogeny\" in experiment:\n",
    "                age = None\n",
    "            elif \"ontogeny\" in experiment:\n",
    "                age = path.parents[2].name.split(\"_\")[0]\n",
    "            else:\n",
    "                age = None\n",
    "            date = datetime.strptime(\n",
    "                path.parents[1].name.split(\"_\")[-1], \"%Y%m%d%H%M%S\"\n",
    "            )\n",
    "            try:\n",
    "                _df = pd.DataFrame(\n",
    "                    dict(\n",
    "                        experiment=experiment,\n",
    "                        file=str(path),\n",
    "                        syllables=h5f[uuid][()],\n",
    "                        date=date,\n",
    "                        uuid=uuid,\n",
    "                        age=age,\n",
    "                        session_name=session_name,\n",
    "                        subject_name=subject_name,\n",
    "                        timestamps=filled_ts - filled_ts[0],\n",
    "                        **filled_scalars,\n",
    "                        **recon_scalars,\n",
    "                    )\n",
    "                )\n",
    "                _df = _df.astype(\n",
    "                    dict(\n",
    "                        syllables=\"int16[pyarrow]\",\n",
    "                        file=\"string[pyarrow]\",\n",
    "                        experiment=\"string[pyarrow]\",\n",
    "                        uuid=\"string[pyarrow]\",\n",
    "                        session_name=\"string[pyarrow]\",\n",
    "                        subject_name=\"string[pyarrow]\",\n",
    "                        timestamps=\"float32[pyarrow]\",\n",
    "                    )\n",
    "                )\n",
    "                df.append(_df)\n",
    "                if i % 35 == 0:\n",
    "                    pd.concat(df, ignore_index=True).to_parquet(\n",
    "                        data_folder / f\"{experiment}_syllable_df.parquet\"\n",
    "                    )\n",
    "            except ValueError:\n",
    "                failed_sessions.append((uuid, path))\n",
    "        df = pd.concat(df, ignore_index=True)\n",
    "        df.to_parquet(data_folder / f\"{experiment}_syllable_df.parquet\")\n",
    "        print(experiment, \"length\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93ced12-524e-4b69-a506-03c7ea2f8e40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35954\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(syllable_path, \"r\") as h5f:\n",
    "    print(len(h5f[uuid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a03b7c8-3e40-4576-bf34-7c11b14dc8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2324"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uuid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e3662ac-3e29-419f-b753-e49b93358866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37649629 entries, 0 to 37649628\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   experiment      string        \n",
      " 1   file            string        \n",
      " 2   syllables       int16[pyarrow]\n",
      " 3   date            datetime64[ns]\n",
      " 4   uuid            string        \n",
      " 5   age             object        \n",
      " 6   session_name    string        \n",
      " 7   subject_name    string        \n",
      " 8   timestamps      float[pyarrow]\n",
      " 9   area_mm         float32       \n",
      " 10  centroid_x_mm   float32       \n",
      " 11  centroid_y_mm   float32       \n",
      " 12  height_ave_mm   float32       \n",
      " 13  length_mm       float32       \n",
      " 14  velocity_2d_mm  float32       \n",
      " 15  velocity_3d_mm  float32       \n",
      " 16  width_mm        float32       \n",
      " 17  angle           float32       \n",
      " 18  velocity_theta  float32       \n",
      " 19  recon_width     float64       \n",
      " 20  recon_length    float64       \n",
      " 21  recon_height    float64       \n",
      " 22  recon_area      float64       \n",
      "dtypes: datetime64[ns](1), float32(10), float64(4), float[pyarrow](1), int16[pyarrow](1), object(1), string(5)\n",
      "memory usage: 10.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56049a88-f07f-4d21-aadb-e5109e7548e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ontogeny",
   "language": "python",
   "name": "aging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "034b97a631874b7e8d0a82a216bc9b57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d989578c02ed4c0ab8817b4b886810b3",
       "max": 1,
       "style": "IPY_MODEL_ea3c81fdecb54b8d9bc2904aac0a9871",
       "value": 1
      }
     },
     "0c16e1366b9d4cfbb46186a2c1b67d33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e42bde8818fd42eaa03f583fb188ff79",
        "IPY_MODEL_31a86f1250ac4a008bfcdd0366fb89f1",
        "IPY_MODEL_41d052f17e8943e482e3df5a756c417f"
       ],
       "layout": "IPY_MODEL_3f28b17b920d48bfaaad144a1e8101bc"
      }
     },
     "0d17bfdd4dab4c88b8c67f45375240a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d4d5c256f1348c6b98e9060b0c1b93b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "11ba96abd594471b91bc37f16aef28be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1d9410c12fef4ffbae35940baacfe4f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1dbdd25c99d84402938f4d52d0ec328a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3f0d2a325c494ac3a51bf7af8cdafd85",
       "style": "IPY_MODEL_0d17bfdd4dab4c88b8c67f45375240a3"
      }
     },
     "24359300c750432194ac9561d3b67ca0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "31a86f1250ac4a008bfcdd0366fb89f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_6627f127c2834585916a2c9fb22dd3d0",
       "max": 1096,
       "style": "IPY_MODEL_24359300c750432194ac9561d3b67ca0",
       "value": 1096
      }
     },
     "3f0d2a325c494ac3a51bf7af8cdafd85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3f28b17b920d48bfaaad144a1e8101bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41d052f17e8943e482e3df5a756c417f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1d9410c12fef4ffbae35940baacfe4f6",
       "style": "IPY_MODEL_fbf785df88164585819bc40512e3c3ca",
       "value": " 1096/1096 [3:41:03&lt;00:00, 10.24s/it]"
      }
     },
     "4678b62cea474cb79a60753f1921acb1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6627f127c2834585916a2c9fb22dd3d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "679b8b7af18844198f727ace8ad43611": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1dbdd25c99d84402938f4d52d0ec328a",
        "IPY_MODEL_034b97a631874b7e8d0a82a216bc9b57",
        "IPY_MODEL_edc0e88c42a446c0a91a7c70b74c7e01"
       ],
       "layout": "IPY_MODEL_4678b62cea474cb79a60753f1921acb1"
      }
     },
     "b669f640aca146bab581f7927e8a0a0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d989578c02ed4c0ab8817b4b886810b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "20px"
      }
     },
     "e42bde8818fd42eaa03f583fb188ff79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0d4d5c256f1348c6b98e9060b0c1b93b",
       "style": "IPY_MODEL_f33794fe79f1458eadabeee8f8509679",
       "value": "100%"
      }
     },
     "ea3c81fdecb54b8d9bc2904aac0a9871": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "edc0e88c42a446c0a91a7c70b74c7e01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_11ba96abd594471b91bc37f16aef28be",
       "style": "IPY_MODEL_b669f640aca146bab581f7927e8a0a0d",
       "value": " 2324/? [00:39&lt;00:00, 63.75it/s]"
      }
     },
     "f33794fe79f1458eadabeee8f8509679": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fbf785df88164585819bc40512e3c3ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
